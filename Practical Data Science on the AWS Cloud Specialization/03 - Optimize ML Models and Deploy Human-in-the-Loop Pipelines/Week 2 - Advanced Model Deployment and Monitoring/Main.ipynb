{"cells":[{"cell_type":"markdown","metadata":{},"source":["# **Advanced Model Deployment and Monitoring**"]},{"cell_type":"markdown","metadata":{},"source":["## **Model Deployment and Integration**"]},{"cell_type":"markdown","metadata":{},"source":["### **Model Deployment Overview**"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-11-12-38.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-11-12-59.png)"]},{"cell_type":"markdown","metadata":{},"source":["There are two general options that you'll learn about including real time Inference and batch inference, I'll cover both of these options this week. But let's start with deploying a model for real time inference in the cloud deploying a model for real time. Inference means deploying it to a persistent hosted environment that's able to serve requests for prediction and provide prediction responses back in real time or near real time. This involves exposing an endpoint that has his serving stack that can accept and respond to requests. A serving stack needs to include a proxy that can accept incoming requests and direct them to an application that then uses your Inference code to interact with your model. This is a good option when you need to have low latency combined with the ability to serve new prediction requests that come in, so some example use cases here would be fraud detection. Where you may need to be able to identify whether an incoming transactions is potentially fraudulent in near real time or product recommendations. Where you want to be able to predict the appropriate products based on a customer's current search history or a customer's current shopping cart. So let's take a look at how a real time persistent endpoint would apply to your product review use case. In this case, let's assume you need to identify whether a product review is negative and immediately notify a customer support engineer about negative reviews. So that they can proactively reach out to the customer right away here you have some type of web application that a consumer enters their product review into. Then that web application or secondary process called by that web application coordinates a call to your real time end point that serves your model with the new product review text. The hosted model then returns a prediction. So in this case it would be a negative class for sentiment that can then be used to initiate a back end process that opens a high severity support ticket to the customer support engineer. Given that your objective here is to have quick customer support response. You can see where you would need to have that model consistently available through a real time endpoint that's able to serve your prediction requests that come in. And serve your response traffic, let's now look at batch inference and see how it compares to real time inference with batch inference. You aren't hosting a model that persists and can serve requests for prediction as they come in. Instead, your batch in those requests for prediction, running a batch job against those batch requests and then out putting your prediction responses typically is batch records as well. Then once you have your prediction responses, they can then be used in a number of different ways. Those prediction responses are often used for reporting or are persisted into a secondary data store for use by other applications or for additional reporting. Use cases that are focused on forecasting are a natural fit for batch inference. So say you're doing sales forecasting where you typically use batch sales data over a period of time to come up with new sales forecast. "]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-11-15-20.png)"]},{"cell_type":"markdown","metadata":{},"source":["In this case, you'd use batch jobs to process those prediction requests and potentially store those predictions for additional visibility or analysis, let's go back to your product review case. So let's say your ultimate business goal here is to be able to identify vendors that have potential quality issues by detecting trends for negative product reviews per vendor. So in this case, you don't need a real time end point, but you would use a batch inference job to take a batch of product review data. Then run batch jobs at a reasonable frequency that you identify that can take all of those product reviews on input. Process those predictions and that output that data just as the prediction request data is a set of batch records on input. The prediction responses that are output to the model are also collected as a collection of batch records. That data could then be persisted so that your analysts could aggregate the data. Run reports to identify any potential issues with vendors that have a large number of negative reviews with your batch job. These jobs aren't persisted so they run for only the amount of time that it takes to process those batch requests on input. Let's briefly touched on deployment to the edge which is an option that is not a cloud specific. But is a key consideration when deploying models closer to your users or in areas with poor network connectivity. In the case of edge deployments, you train your models in another environment in this case in the cloud and then optimize your model for deployment to edge devices. This process is typically aimed at compiling or packaging your model in a way that is optimized to run at the edge. Which usually means things like reducing the model package size for running on smaller devices. In this case you could use something like Sagemaker Neo to compile your model in a way that is optimized for running at the edge and use cases. Bring your model closer to where it will be used for prediction, so typical use cases here would be like manufacturing, where you have cameras on an assembly line. And you need to make real time inferences or in use cases where you need to detect equipment anomalies at the edge. Inference data in this case is often sent back to the cloud for additional analysis or for collection of ground truth data that can then be used to further optimize your model. I discovered the primary options for deploying a model covering real time Inference, batch inference and deploying models to the edge, the right option to choose depends on several factors. The choice to deploy to the edge is typically an obvious one as there's edge devices and you might be working with use cases where there is limited network connectivity. You might also be working with internet of things or IOT use cases or use cases where the cost in terms of the time spent in data transfer is not an option even when it's single digit millisecond response. Now, the choice between real time inference and batch inference typically comes down to the ways that you need to request and consume predictions in combination with cost. A real time endpoint can serve real time predictions, where the prediction requests sent on input is unique and requires an immediate response with low latency. The trade off is that a persistent endpoint typically cost more because you pay for the compute. And the storage resources that are required to host that model while that endpoint is up and running a batch job in contrast works well when you can batch your data for prediction. And that's your responses back, now, these responses can then be persisted into a secondary database that can serve real time applications when there is no need for new prediction requests. And responses per transaction, so in this case, you can run batch jobs in a transient environment. Meaning that the compute and storage environments are only active for the duration of your batch job. As a general rule, you should use the option that meets your use case and is the most cost effective. "]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-11-18-22.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-11-18-57.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-11-19-20.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-11-19-50.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-11-20-32.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-11-21-10.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-11-21-40.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-11-22-01.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-11-22-28.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-11-23-01.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-11-23-41.png)"]},{"cell_type":"markdown","metadata":{},"source":["### **Model Deployment Strategies**"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-11-36-18.png)"]},{"cell_type":"markdown","metadata":{},"source":["This is important because you want to be able to deploy new models in a way that minimizes risk and downtime while measuring the performance of a new model or a new model version. As an example, if you have a newer version of a model, you typically don't want to deploy that model or that new version in a way that disrupts service. You may also want to monitor the performance of that new model version for a period of time in a way that allows you to seamlessly roll back if there is an issue with that new version. In this section, I'll talk about some of the common deployment strategies. I'll cover each of these deployment strategies so you'll learn about blue/green deployments, shadow/challenger deployments, canary deployments, A/B testing, and finally, multi-armed bandits. I'll start with blue/green deployments. Some of you may be familiar with the concept of blue/green deployments for applications or software. The same concept really applies to models as well. With blue/green deployments, you deploy your new model version to a stack that conserved prediction and response traffic coming into an endpoint. Then when you're ready to have that new model version actually start to process prediction requests coming in, you swap the traffic to that new model version. This makes it easy to roll back because if there are issues with that new model or that new model version doesn't perform well, you can swap traffic back to the previous model version. Let's take a closer look at how a blue/green deployment works. With blue/green deployment, you have a current model version running in production. In this case, we have version 1. This accepts 100 percent of the prediction request traffic and responds with prediction responses. When you have a new model version to deploy, in this case, model version 2, you build a new server or container to deploy your model version into. This includes not only the new model version but also the code in the software that's needed to accept and respond to prediction requests. As you can see in this picture, the new model version is deployed, but the load balancer has not yet been updated to point to that new server hosting the model, so no traffic is hitting that endpoint yet. After the new model version is deployed successfully, you can then shift 100 percent of your traffic to that new cluster serving model version 2 by updating your load balancer. This strategy helps reduce downtime if there's a need to roll back and swap back to version 1 because you only need to re-point your load balancer back to version 1. The downside to this strategy is that it is 100 percent swap of traffic. So if the new model version, version 2, in this case, is not performing well, then you run the risk of serving bad predictions to 100 percent of your traffic versus a smaller percentage of traffic. Let's now cover the second type of deployment strategy you see here called shadow or challenger deployment. This is often referred to as challenger models because in this case, you're running a new model version in production by letting the new version accept prediction requests to see how that new model would respond, but you're not actually serving the prediction response data from that new model version. This lets you validate the new model version with real traffic without impacting live prediction responses. Let's take a look at how it works. You can see with the shadow or challenger deployment strategy, the new model version is deployed and both versions have 100 percent of prediction requests traffic being sent to each version. However, you'll notice for version 2, only the prediction requests are sent to the model, and you aren't actually serving prediction responses from model version 2. Responses that would have been sent back for model version 2 are typically captured and then analyzed for whether version 1 or version 2 of the model would have performed better against that full traffic load. This strategy also allows you to minimize the risk of deploying a new model version that may not perform as well as model version 1, and this is because you're still able to analyze how version 2 of your model would perform without actually serving the prediction responses back from that model version. Then once you are comfortable that model version 2 is performing better, you can actually start to serve prediction responses directly from model version 2 instead of model version 1. The next deployment strategy that I'll cover is canary deployment. With a canary deployment, you split traffic between model versions and target a smaller group to expose that new model version 2. Typically, you're exposing the select set of users to the new model for a smaller period of time to be able to validate the performance of that new model version before fully deploying that new version out to production. Canary deployment is a deployment strategy where you're essentially splitting traffic between two model versions, and again, with canary deployments, you typically expose a smaller specific group to that new model version while model version 1 still serves the majority of your traffic."]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-11-41-46.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-11-43-08.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-11-44-26.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-11-44-56.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-11-45-45.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-11-49-58.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-11-50-42.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-11-53-10.png)"]},{"cell_type":"markdown","metadata":{},"source":["In the image here you can see that 95 percent of prediction requests and responses are served by Model Version 1 and a smaller set of users are directed to Model Version 2. Canary deployments are good for validating a new model version with a specific or smaller set of users before rolling it out to all users. This is something that can't be done with a blue-green deployment strategy. The next deployment strategy is A/B testing. Canary and A/B testing are similar in that you're splitting traffic. However, A/B testing is different in that typically you're splitting traffic between larger groups and for longer periods of time to measure performance of different model versions over time. This split can be done by targeting specific user groups or just by setting a percentage of traffic to randomly distribute to different groups. Let's take a closer look at A/B testing. With A/B testing, again, you're also splitting your traffic to compare model versions. However, here you split traffic between those larger groups for the purpose of comparing different model versions in live production environments. Here, you typically do a larger split across users. So 50 percent one model version, 50 percent the other model version. Then you can also perform A/B testing against more than two model versions as well, although it's not shown here. While A/B testing seemed similar to canary deployments, A/B testing tests those larger groups, like I mentioned, and typically runs for longer periods of time than canary deployments. A/B tests are focused on gathering live data about different model versions. They typically, again, run for longer periods of time to be able to gather that performance data that is statistically significant enough, which provides that ability to confidently roll out Version 2 to a larger percent of traffic. Because you're running multiple models for longer periods of time, A/B testing allows you to really validate your different model versions over multiple variations of user behavior. As an example, you may have a forecasting use case that has seasonality to it. You need to be able to capture how your model performs over changes to the environment over time. So I just covered some of the common static approaches to deploying new or updated models. All of the approaches that were covered are static approaches, meaning that you manually identify things like when to swap traffic and how to distribute that traffic. I'll cover another approach that is more dynamic in nature, meaning that instead of manually identify when and how you distribute traffic, you can take advantage of approaches that incorporate machine learning to automatically decide when and how to distribute traffic between multiple versions of a deployed model. For this, I'll cover multi-armed bandits. A/B tests are typically fairly static and need to run over a period of time. With this, you do run the potential risk of running with a bad or low-performing model for that same longer period of time. A more dynamic method for testing is multi-armed bandits. Multi-armed bandits use reinforcement learning as a way to dynamically shift traffic to the winning model versions by rewarding the winning model with more traffic but still exploring the nonwinning model versions in the case that those early winners were not the overall best models. Let's take a look at what multi-armed bandit strategy testing looks like. In this implementation, you first have an experiment manager, which is basically a model that uses reinforcement learning to determine how to distribute traffic between your model versions. This model chooses the model version to send traffic to based on the current reward metrics and the chosen exploit explore strategy. Exploitation refers to continuing to send traffic to that winning model, whereas exploration allows for routing traffic to other models to see if they can eventually catch up or perform as well as the other model. It will also continue to adjust that prediction traffic to send more traffic to the winning model. In this example, you can see a new product review and star rating comment, and in this case, your model versions are trying to predict the star rating. You can see Model Version 1 predicted that this was a five-star rating, while Model Version 2 predicted it was a four-star rating. The actual rating was four stars. So in this case Model Version 2 wins. So your multi-arm bandit will reward that model by sending more traffic to Model Version 2. In this section, you learned about various deployment strategies that can be used to minimize downtime and evaluate the performance of a new model with no or minimal impact to your users. All of these concepts are general and they cover machine learning on any platform. But in the next section you'll learn more about deployment options that are really specific to Amazon SageMaker."]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-11-55-57.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-11-57-05.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-11-58-07.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-11-59-28.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-12-00-28.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-12-02-16.png)"]},{"cell_type":"markdown","metadata":{},"source":["### **Amazon SageMaker Hosting: Real-Time Inference**"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-12-07-16.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-12-08-11.png)"]},{"cell_type":"markdown","metadata":{},"source":["Let's go a little deeper and talk more about those endpoints and some of their more advanced features. As a reminder, SageMaker endpoints can be used to serve your models for predictions in real-time with low latency. Serving your predictions in real-time requires a model serving stack that not only has your trained model, but also a hosting stack to be able to serve those predictions. That hosting stack typically include some type of a proxy, a web server that can interact with your loaded serving code and your trained model. Your model can then be consumed by client applications through real time invoke API request. The request payload sent when you invoke the endpoint is routed to a load balancer and then routed to your machine learning instance or instances that are hosting your models for prediction. SageMaker has several built-in serializers and deserializers that you can use depending on your data formats. As an example for serialization on prediction request, you can use the JSON line serializer, which will then serialize your inference requests data to a JSON lines formatted string. For deserialization on prediction response, the JSON deserializer will then deserialize JSON lines data from an inference endpoint response. Finally, response payload is then routed back to the client application. With SageMaker model hosting, you choose the machine-learning instance type, as well as the count combined with the docker container image and optionally the inference code, and then SageMaker takes care of creating the endpoint, and deploying that model to the endpoint. The type of machine learning instance you choose really comes down to the amount of compute and memory you need. I discovered the high-level architecture with deployed SageMaker endpoint, but let's now cover some of the deployment options as they relate to the actual components that are deployed inside your machine learning instance. SageMaker has three basic scenarios for deployment when you use it to train and deploy your model. You can use prebuilt code, prebuilt serving containers, or a mixture of the two. I'll start with deploying a model that was trained using a built-in algorithm. In this option, you use both prebuilt inference code combined with a prebuilt serving container. The container includes the web proxy and the serving stack combined with the code that's needed to load and serve your model for real time predictions. This scenario would be valid for some of the SageMaker built-in algorithms where you need only your trained model and the configuration for how you want to host that machine learning instance behind that endpoint. For this scenario to deploy your endpoint, you identify the prebuilt container image to use and then the location of your trained model artifact in S3. Because SageMaker provides these built-in container images, you don't have any container images to actually build for this scenario. Let's now cover deploying a model using a built-in framework like TensorFlow or PyTorch, where you're still using prebuilt container images for inference, but with the option of bringing your own serving code as well. The next option still uses a prebuilt container that's purpose-built for a framework such as TensorFlow or PyTorch, and then you can optionally bring your own serving code. In this option, you'll notice that while you're still using a prebuilt container image, you may still need or want to bring your own inference code. You'll have the opportunity to specifically work with this option in the lab for this week. Finally, the last optional cover is bringing your own container image and inference code for hosting a model on a SageMaker endpoint. In this case, you'll have some additional work to do by creating a container that's compatible with SageMaker for inference. "]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-12-18-27.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-12-20-30.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-12-21-04.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-12-21-39.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-12-22-24.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-12-22-46.png)"]},{"cell_type":"markdown","metadata":{},"source":["k![](2024-01-02-12-22-59.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-12-23-18.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-12-23-51.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-12-24-35.png)"]},{"cell_type":"markdown","metadata":{},"source":["But this also offers the flexibility to choose and customize the underlying container that's hosting your model. You just learned about the three different types of deployment options for SageMaker. All of these options deploy your model to a number of machine learning instances that you specify when you're configuring your endpoint. You typically want to use smaller instances and more than one machine learning instance. In this case, SageMaker will automatically distribute those instances across AWS availability zones for high availability. But once your endpoints are deployed, how do you then ensure that you're able to scale up and down to meet the demands of your workloads without overprovisioning your ML instances. This is where autoscaling comes in. It allows you to scale the number of machine learning instances that are hosting your endpoints up or down based on your workload demands. This is important to meet the demands of your workload, which means that you can increase the number of instances that serve your model when you reach a threshold for capacity that you've established. This is also important for cost optimization for two reasons. First, not only can you scale your instances up to meet the higher workload demands when you need it, but you can also scale it back down to a lower level of compute when it is no longer needed. Second, using autoscaling allows you to maintain a minimum footprint during normal traffic workloads, versus overprovisioning and paying for compute that you don't need. The on-demand access to compute and storage resources that the Cloud provides allows for this ability to quickly scale up and down. Let's take a look at how it works conceptually. When you deploy your endpoint, the machine learning instances that back that implant will emit a number of metrics to Amazon CloudWatch. For those that are unfamiliar with it, CloudWatch is the managed AWS service for monitoring your AWS resources. SageMaker emits a number of metrics about that deployed endpoints such as utilization metrics and invocation metrics. Invocation metrics indicate the number of times an invoke endpoint request has been run against your endpoint, and it's the default scaling metric for SageMaker autoscaling. You can actually define a custom scaling metric as well, such as CPU utilization. Let's assume you've set up your autoscaling on your endpoint and you're using the default scaling metric of number of invocations. Each instance will emit that metric to CloudWatch. As part of the scaling policy that you can figure. If the number of invocations exceeds the threshold that you've identified, then SageMaker will apply the scaling policy and scale up by the number of instances that you've configured. After scaling policy for your endpoint, the new instances will come online and your load balancer will be able to distribute traffic load to those new instances automatically. You can also add a cool down policy for scaling out your model, which is the value in seconds that you specify to wait for a previous scaled-out activity to take effect. The scale out cooldown period is intended to allow instances to scale out continuously, but not excessively. Finally, you can specify a cool down period for scaling in your model as well. This is the amount of time in seconds, again, after a scale-in activity completes, before another scale-in activity can start. This allows instances to scale in slowly. I just covered the concept of autoscaling SageMaker endpoints, but let's now cover how you actually set it up. First, you register your scalable target. A scalable target is an AWS resource, and in this case, you want to scale the SageMaker resource as indicated in the service namespace. This is accepted as your input parameter. Because autoscaling is used by other AWS resources, you'll see a few parameters that specifically indicate that you want to scale a SageMaker endpoint resource. Similarly, the scalable dimension is a set value for SageMaker endpoint scaling. Some of the additional input parameters that you need to configure include the resource ID, which in this case is the endpoint variant that you want to scale. You'll also need to specify a few key parameters that control the minimum and maximum number of machine learning instances. The minimum capacity indicates the minimum value you plan to scale into. The maximum capacity is the maximum number of instances that you want to scale out to. In this case, you always want to have at least one instance running, and a maximum of two during peak periods. After you register your scalable target, you need to then define the scaling policy. The scaling policy provides additional information about the scaling behavior for your instances. In this case, you have your predefined metric, which is the number of invocations on your instance, and then your target value, which indicates the number of invocations per machine learning instance that you want to allow before invoking your scaling policy. You'll also see the scale-out and scale-in cooldown metrics that I mentioned previously. In this case, you see a scale-out cooldown of 60, which means that after autoscaling successfully scales out, it starts to calculate that cool-down time. The Scaling policy will increase again to that desired capacity until the cool down period ends. The ScaleInCool down setting of 300 seconds means a SageMaker will not attempt to start another cool down policy within 300 seconds when the last one completed. In your final step to set up autoscaling, you will apply autoscaling policy, which means you apply that policy to your endpoint. "]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-12-25-14.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-12-26-21.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-12-27-06.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-12-27-53.png)"]},{"cell_type":"markdown","metadata":{},"source":["Your endpoint will now be skilled in and scaled out according to that scaling policy that you've defined. You'll notice here you refer to the previous configuration that was discussed, and you'll also see a new parameter called policy type. Target tracking scaling refers to the specific autoscaling type that is supported by SageMaker. This uses a scaling metric and a target value as an indicator to scale. You'll have the opportunity to get hands on your lab for this week in setting up and applying autoscaling to SageMaker endpoints. You just learned about how SageMaker handles deployment to your machine-learning instances across a variety of options, and I also walked you through how to apply autoscaling to dynamically provision resources to meet the demands of your workload. But I'll quickly cover a few additional capabilities for SageMaker endpoints that you should be aware of, including multi-model endpoints and inference pipelines. I'll start with multi-model endpoints. Until now, you've learned about SageMaker endpoints that serve predictions for one model. However, you can also host multiple models behind a single endpoint. Instead of downloading your model from S3 to them machine learning instance immediately when you create the endpoint, with multi-model endpoints, SageMaker dynamically loads your models when you invoke them. You invoke them through your client applications by explicitly identifying the model that you're invoking. In this case you see the predict function is identifying Model 1 for this prediction request. SageMaker will keep that model loaded until resources are exhausted on that instance. If you remember, I previously discussed the deployment options around the container image that is used for inference when you deploy a SageMaker endpoint. All of the models that are hosted on a multi-modal endpoint must share the same serving container image. Multi-model endpoints are an option that can improve endpoint utilization when your models are of similar size and share the same container image and have similar invocation latency requirements. Here, you'll see another feature called inference pipeline. Inference pipeline allows you to host multiple models behind a single endpoint. But in this case, the models are sequential chain of models with the steps that are required for inference. This allows you to take your data transformation model, your predictor model, and your post-processing transformer, and host them so they can be sequentially run behind a single endpoint. As you can see in this picture, the inference request comes into the endpoint, then the first model is invoked, and that model is your data transformation. The output of that model is then passed to the next step, which is actually your XGBoost model here, or your predictor model. That output is then passed to the next step, where ultimately in that final step in the pipeline, it provides the final response or the post-process response to that inference request. This allows you to couple your pre and post-processing code behind the same endpoint and helps ensure that your training and your inference code stay synchronized. In this section, you learned more about using SageMaker Hosting to deploy models, do a fully managed endpoint for your real-time inference use cases. You also learned about hosting your endpoint on machine learning instances where you can take advantage of capabilities like autoscaling to dynamically increase or decrease the number of machine learning instances hosting your models so that they can meet the demands of your prediction request traffic. You also learned about some of the advanced deployment options such as multi-model endpoints and inference pipeline. These won't be in your labs for this week, but they are advanced deployment options to be aware of when you're looking at the best option for deploying your models."]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-12-28-42.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-12-29-13.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-12-29-59.png)"]},{"cell_type":"markdown","metadata":{},"source":["### **Amazon SageMaker: Real-time Inference Production Variants**"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-12-36-40.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-12-36-57.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-12-37-24.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-12-37-54.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-12-38-35.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-12-39-01.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-12-39-22.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-12-39-43.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-12-39-54.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-12-40-23.png)"]},{"cell_type":"markdown","metadata":{},"source":["### **Amazon SageMaker Batch Transform: Batch Inference**"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-12-42-26.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-12-42-45.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-12-43-57.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-12-44-14.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-12-44-43.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-12-44-54.png)"]},{"cell_type":"markdown","metadata":{},"source":["Let's start with how batch Transform jobs work. We've batch Transform, you package your model first. This step is the same, whether you're going to deploy your model to a SageMaker endpoint, or whether you're deploying it for batch use cases. Similar to hosting for SageMaker endpoints, you either use a built-in container for your inference image or you can also bring your own. Your model package contains information about the S3 location of your trained model artifact, and the container image to use for inference. Next, you create your transformer. For this, you provide the configuration information about how you want your batch job to run. This also includes parameters such as the size and the type of machine learning instances that you want to run your batch job with, as well as the name of the model package that you previously created. Additionally, you specify the output location, which is the S3 bucket, where you want to store your prediction responses. After you've configured your transformer, you're ready to start your batch transformed job. This can be done on an ad hoc basis or scheduled as part of a normal process. When you start your job, you provide the S3 location of your batch prediction requests data. SageMaker will then automatically spin up the Machine Learning instances using the configuration that you supplied, and it will process you're batch requests for prediction. When the job is complete, SageMaker will automatically output the prediction response data to the S3 location that you specified and spin down the Machine Learning Instances. Batch jobs operate in a transient environment, which means that the Compute is only needed for the time it takes to complete the batch Transform job. Batch Transform also has more advanced features such as inference pipeline. If you recall, inference pipeline allows you to sequentially chain together multiple models. You can combine your steps for inference within a single batch job so that the batch job includes your data transformation model for transforming your input data into the format expected by the model, the actual model for prediction, and then potentially a data post-processing model that transforms the labels that will be used as your inference response and put to your S3 bucket for output. In this section, you learned about batch Transform as a way to deploy your model using SageMaker so that it meets your batch use case needs. You also learn that similar to SageMaker endpoints, you can use the feature called inference pipeline to combine multiple models to run sequentially during your batch job. "]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-12-45-32.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-12-45-53.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-01-02-12-47-22.png)"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["## **Model Integration and Monitoring**"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNAnc3IBjNxONndNFHGocAe","collapsed_sections":[],"name":"M1Assignment1 Solution.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":0}
