{"cells":[{"cell_type":"markdown","metadata":{},"source":["# **Week Topic: Reinforcement learning and LLM-powered applications**"]},{"cell_type":"markdown","metadata":{},"source":["# **Reinforcement learning for human feedback**"]},{"cell_type":"markdown","metadata":{},"source":["## **Aligning models with human values**"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-21-23-46-25.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-21-23-47-00.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-21-23-48-39.png)"]},{"cell_type":"markdown","metadata":{},"source":["## **Reinforcement learning from human feedback (RLHF)**"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-00-04-30.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-00-05-23.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-00-06-11.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-00-07-33.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-00-09-07.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-00-09-46.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-00-10-13.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-00-21-44.png)"]},{"cell_type":"markdown","metadata":{},"source":["## **RLHF: Obtaining feedback from humans**"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-00-23-19.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-00-26-24.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-00-26-48.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-00-28-56.png)"]},{"cell_type":"markdown","metadata":{},"source":["## **RLHF: Reward model**"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-00-50-40.png)"]},{"cell_type":"markdown","metadata":{},"source":["For a given prompt X, the reward model learns to favor the human-preferred completion y_ j, while minimizing the lock sigmoid off the reward difference, r_j-r_k. "]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-01-00-35.png)"]},{"cell_type":"markdown","metadata":{},"source":["## **RLHF: Fine-tuning with reinforcement learning**"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-01-12-34.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-01-13-02.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-01-13-55.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-01-14-36.png)"]},{"cell_type":"markdown","metadata":{},"source":["## **Proximal policy optimization**"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-01-27-33.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-01-28-35.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-01-29-10.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-01-29-58.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-01-30-16.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-01-31-02.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-01-40-33.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-01-41-05.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-02-00-32.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-02-01-56.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-02-02-17.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-02-02-50.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-02-03-08.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-02-03-59.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-02-04-08.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-02-04-37.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-02-04-55.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-02-05-09.png)"]},{"cell_type":"markdown","metadata":{},"source":["Q-learning is an alternate technique for fine-tuning LLMs through RL, but PPO is currently the most popular method. In my opinion, PPO is popular because it has the right balance of complexity and performance. That being said, fine-tuning the LLMs through human or AI feedback is an active area of research. We can expect many more developments in this area in the near future. For example, just before we were recording this video, researchers at Stanford published a paper describing a technique called direct preference optimization, which is a simpler alternate to RLHF. New methods like this are still in active development, and more work has to be done to better understand their benefits, but I think this is a very exciting area of research. "]},{"cell_type":"markdown","metadata":{},"source":["## **RLHF: Reward hacking**"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-02-14-24.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-02-15-03.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-02-15-51.png)"]},{"cell_type":"markdown","metadata":{},"source":["To prevent our board hacking from happening, you can use the initial instruct LLM as performance reference. Let's call it the reference model. The weights of the reference model are frozen and are not updated during iterations of RHF. This way, you always maintain a single reference model to compare to. During training, each prompt is passed to both models, generating a completion by the reference LLM and the intermediate LLM updated model. At this point, you can compare the two completions and calculate a value called the Kullback-Leibler divergence, or KL divergence for short. KL divergence is a statistical measure of how different two probability distributions are. You can use it to compare the completions off the two models and determine how much the updated model has diverged from the reference. Don't worry too much about the details of how this works. The KL divergence algorithm is included in many standard machine learning libraries and you can use it without knowing all the math behind it. You'll actually make use of KL divergence in this week's lab so you can see how this works for yourself. KL divergence is calculated for each generate a token across the whole vocabulary off the LLM. This can easily be tens or hundreds of thousands of tokens. However, using a softmax function, you've reduced the number of probabilities to much less than the full vocabulary size. Keep in mind that this is still a relatively compute expensive process. "]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-02-33-46.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-02-34-20.png)"]},{"cell_type":"markdown","metadata":{},"source":["You will almost always benefit from using GPUs. Once you've calculated the KL divergence between the two models, you added acid term to the reward calculation. This will penalize the RL updated model if it shifts too far from the reference LLM and generates completions that are two different. Note that you now need to full copies of the LLM to calculate the KL divergence, the frozen reference LLM, and the oral updated PPO LLM. By the way, you can benefit from combining our relationship with puffed. In this case, you only update the weights of a path adapter, not the full weights of the LLM. This means that you can reuse the same underlying LLM for both the reference model and the PPO model, which you update with a trained path parameters. This reduces the memory footprint during training by approximately half. I know that there is a lot to take in here, but don't worry, RHF with path is going to be covered in the lab. If you'll get a chance to see this in action and try it out for yourself. Once you have completed your RHF alignment of the model, you will want to assess the model's performance. You can use the summarization data set to quantify the reduction in toxicity, for example, the dialogue, some data set that you saw earlier in the course. The number you'll use here is the toxicity score, this is the probability of the negative class, in this case, a toxic or hateful response averaged across the completions. If RHF has successfully reduce the toxicity of your LLM, this score should go down. First, you'll create a baseline toxicity score for the original instruct LLM by evaluating its completions off the summarization data set with a reward model that can assess toxic language. Then you'll evaluate your newly human aligned model on the same data set and compare the scores. In this example, the toxicity score has indeed decreased after Arlo HF, indicating a less toxic, better aligned model. "]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-02-42-47.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-02-43-22.png)"]},{"cell_type":"markdown","metadata":{},"source":["## **KL divergence**"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-02-45-46.png)"]},{"cell_type":"markdown","metadata":{},"source":["## **Scaling human feedback**"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-02-47-06.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-02-47-56.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-02-48-16.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-02-48-55.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-02-49-33.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-02-49-58.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-02-50-38.png)"]},{"cell_type":"markdown","metadata":{},"source":["## **Lab 3 walkthrough**"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-04-10-08.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-04-10-18.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-04-10-43.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-04-12-00.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-04-12-34.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-04-13-17.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-04-13-34.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-04-13-58.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-04-14-29.png)"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-04-14-45.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-04-14-58.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-04-15-25.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-04-15-51.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-04-16-11.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-04-16-46.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-04-17-16.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-04-17-46.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-04-18-31.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-04-19-18.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-04-19-35.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-04-20-19.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-04-20-51.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-04-21-08.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-04-21-26.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-04-21-44.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-04-22-05.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-04-22-53.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-04-23-11.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-04-23-36.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-04-24-05.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-04-24-37.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-04-24-58.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-04-25-09.png)"]},{"cell_type":"markdown","metadata":{},"source":["# **LLM Powered Applications**"]},{"cell_type":"markdown","metadata":{},"source":["## **Model optimizations for deployment**"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-23-24-49.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-23-25-40.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-23-26-53.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-23-29-04.png)"]},{"cell_type":"markdown","metadata":{},"source":["With a temperature parameter greater than one, the probability distribution becomes broader and less strongly peaked. This softer distribution provides you with a set of tokens that are similar to the ground truth tokens. In the context of Distillation, the teacher model's output is often referred to as soft labels and the student model's predictions as soft predictions. "]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-23-31-44.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-23-32-35.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-23-33-16.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-23-34-46.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-23-35-32.png)"]},{"cell_type":"markdown","metadata":{},"source":["## **Generative AI Project Lifecycle Cheat Sheet**"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-00-00-22.png)"]},{"cell_type":"markdown","metadata":{},"source":["## **Using the LLM in applications**"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-00-08-58.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-00-10-07.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-00-12-53.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-00-27-12.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-00-31-14.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-00-31-53.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-00-33-02.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-00-34-49.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-00-35-21.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-00-35-46.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-00-36-02.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-00-36-36.png)"]},{"cell_type":"markdown","metadata":{},"source":["## **Interacting with external applications**"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-00-38-25.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-00-38-50.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-00-39-31.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-00-40-15.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-00-41-27.png)"]},{"cell_type":"markdown","metadata":{},"source":["## **Helping LLMs reason and plan with chain-of-thought**"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-01-00-44.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-01-01-54.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-01-04-07.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-01-05-28.png)"]},{"cell_type":"markdown","metadata":{},"source":["## **Program-aided language models (PAL)**"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-01-42-45.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-01-43-59.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-01-44-35.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-01-44-55.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-01-45-12.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-01-45-42.png)"]},{"cell_type":"markdown","metadata":{},"source":["## **ReAct: Combining reasoning and action**"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-01-47-00.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-01-47-25.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-01-48-08.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-01-48-17.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-01-48-47.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-01-49-13.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-01-50-01.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-01-50-49.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-01-51-20.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-01-51-37.png)"]},{"cell_type":"markdown","metadata":{},"source":["## **ReAct: Reasoning and action**"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-01-36-28.png)"]},{"cell_type":"markdown","metadata":{},"source":["## **LLM application architectures**"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-01-39-47.png)"]},{"cell_type":"markdown","metadata":{},"source":["## **AWS Sagemaker JumpStart**"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-01-24-41.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-01-25-35.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-01-26-23.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-01-26-34.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-01-27-09.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-01-27-34.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-01-27-44.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-01-28-05.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-01-28-32.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-01-29-01.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-01-29-18.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-01-29-43.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-23-01-29-58.png)"]},{"cell_type":"markdown","metadata":{},"source":["## **Responsible AI**"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-21-08-55-13.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-21-09-06-33.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-21-09-22-25.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-21-09-30-16.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-21-09-32-42.png)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNAnc3IBjNxONndNFHGocAe","collapsed_sections":[],"name":"M1Assignment1 Solution.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":0}
