{"cells":[{"cell_type":"markdown","metadata":{},"source":["# **Week Topic: Reinforcement learning and LLM-powered applications**"]},{"cell_type":"markdown","metadata":{},"source":["## **Aligning models with human values**"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-21-23-46-25.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-21-23-47-00.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-21-23-48-39.png)"]},{"cell_type":"markdown","metadata":{},"source":["## **Reinforcement learning from human feedback (RLHF)**"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-00-04-30.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-00-05-23.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-00-06-11.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-00-07-33.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-00-09-07.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-00-09-46.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-00-10-13.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-00-21-44.png)"]},{"cell_type":"markdown","metadata":{},"source":["## **RLHF: Obtaining feedback from humans**"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-00-23-19.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-00-26-24.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-00-26-48.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-00-28-56.png)"]},{"cell_type":"markdown","metadata":{},"source":["## **RLHF: Reward model**"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-00-50-40.png)"]},{"cell_type":"markdown","metadata":{},"source":["For a given prompt X, the reward model learns to favor the human-preferred completion y_ j, while minimizing the lock sigmoid off the reward difference, r_j-r_k. "]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-01-00-35.png)"]},{"cell_type":"markdown","metadata":{},"source":["## **RLHF: Fine-tuning with reinforcement learning**"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-01-12-34.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-01-13-02.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-01-13-55.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-01-14-36.png)"]},{"cell_type":"markdown","metadata":{},"source":["## **Proximal policy optimization**"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-01-27-33.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-01-28-35.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-01-29-10.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-01-29-58.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-01-30-16.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-01-31-02.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-01-40-33.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-01-41-05.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-02-00-32.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-02-01-56.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-02-02-17.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-02-02-50.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-02-03-08.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-02-03-59.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-02-04-08.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-02-04-37.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-02-04-55.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-22-02-05-09.png)"]},{"cell_type":"markdown","metadata":{},"source":["Q-learning is an alternate technique for fine-tuning LLMs through RL, but PPO is currently the most popular method. In my opinion, PPO is popular because it has the right balance of complexity and performance. That being said, fine-tuning the LLMs through human or AI feedback is an active area of research. We can expect many more developments in this area in the near future. For example, just before we were recording this video, researchers at Stanford published a paper describing a technique called direct preference optimization, which is a simpler alternate to RLHF. New methods like this are still in active development, and more work has to be done to better understand their benefits, but I think this is a very exciting area of research. "]},{"cell_type":"markdown","metadata":{},"source":["## **Introduction**"]},{"cell_type":"markdown","metadata":{},"source":["## **Introduction**"]},{"cell_type":"markdown","metadata":{},"source":["## **Introduction**"]},{"cell_type":"markdown","metadata":{},"source":["## **Introduction**"]},{"cell_type":"markdown","metadata":{},"source":["## **Introduction**"]},{"cell_type":"markdown","metadata":{},"source":["## **Responsible AI**"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-21-08-55-13.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-21-09-06-33.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-21-09-22-25.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-21-09-30-16.png)"]},{"cell_type":"markdown","metadata":{},"source":["![](2024-04-21-09-32-42.png)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNAnc3IBjNxONndNFHGocAe","collapsed_sections":[],"name":"M1Assignment1 Solution.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":0}
